{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from Bert import BERT\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from Bert import BERT\n",
    "from BertConfig import BertConfig\n",
    "path = \"./config.json\"\n",
    "config = BertConfig(path)\n",
    "bert = BERT(config)\n",
    "\n",
    "torch_list = [\n",
    "\"embeddings.word_embeddings.weight\",\n",
    "\"embeddings.position_embeddings.weight\",\n",
    "\"embeddings.token_type_embeddings.weight\",\n",
    "\"embeddings.LayerNorm.weight\",\n",
    "\"embeddings.LayerNorm.bias\",\n",
    "\"encoder.layers.0.attention.attention.query.weight\",\n",
    "\"encoder.layers.0.attention.attention.query.bias\",\n",
    "\"encoder.layers.0.attention.attention.key.weight\",\n",
    "\"encoder.layers.0.attention.attention.key.bias\",\n",
    "\"encoder.layers.0.attention.attention.value.weight\",\n",
    "\"encoder.layers.0.attention.attention.value.bias\",\n",
    "\"encoder.layers.0.attention.output.dense.weight\",\n",
    "\"encoder.layers.0.attention.output.dense.bias\",\n",
    "\"encoder.layers.0.attention.output.LayerNorm.weight\",\n",
    "\"encoder.layers.0.attention.output.LayerNorm.bias\",\n",
    "\"encoder.layers.0.intermediate.dense.weight\",\n",
    "\"encoder.layers.0.intermediate.dense.bias\",\n",
    "\"encoder.layers.0.output.dense.weight\",\n",
    "\"encoder.layers.0.output.dense.bias\",\n",
    "\"encoder.layers.0.output.LayerNorm.weight\",\n",
    "\"encoder.layers.0.output.LayerNorm.bias\",\n",
    "\"pooler_transform/kernel:0\",\n",
    "\"pooler_transform/bias:0\"\n",
    "]\n",
    "\n",
    "tf_list = [ 'word_embeddings/embeddings:0',\n",
    "    'position_embedding/embeddings:0',\n",
    "    'type_embeddings/embeddings:0',\n",
    "    'embeddings/layer_norm/gamma:0',\n",
    "    'embeddings/layer_norm/beta:0',\n",
    "    'transformer/layer_0/self_attention/query/kernel:0',\n",
    "    'transformer/layer_0/self_attention/query/bias:0',\n",
    "    'transformer/layer_0/self_attention/key/kernel:0',\n",
    "    'transformer/layer_0/self_attention/key/bias:0',\n",
    "    'transformer/layer_0/self_attention/value/kernel:0',\n",
    "    'transformer/layer_0/self_attention/value/bias:0',\n",
    "    'transformer/layer_0/self_attention/attention_output/kernel:0',\n",
    "    'transformer/layer_0/self_attention/attention_output/bias:0',\n",
    "    'transformer/layer_0/self_attention_layer_norm/gamma:0',\n",
    "    'transformer/layer_0/self_attention_layer_norm/beta:0',\n",
    "    'transformer/layer_0/intermediate/kernel:0',\n",
    "    'transformer/layer_0/intermediate/bias:0',\n",
    "    'transformer/layer_0/output/kernel:0',\n",
    "    'transformer/layer_0/output/bias:0',\n",
    "    'transformer/layer_0/output_layer_norm/gamma:0',\n",
    "    'transformer/layer_0/output_layer_norm/beta:0',\n",
    "    'pooler_transform/kernel:0', \n",
    "    'pooler_transform/bias:0']\n",
    "\n",
    "\n",
    "def map_tf_torch_params():\n",
    "    torch_tf_mapping = dict(zip(torch_list, tf_list))\n",
    "    layer_dict = {}\n",
    "    for i in range(1, 12):\n",
    "        for key, value in torch_tf_mapping.items():\n",
    "            if \"layers\" in key:\n",
    "                new_key = key.replace(\"layers.0\", f\"layers.{i}\")\n",
    "                new_value = value.replace(\"layer_0\", f\"layer_{i}\")\n",
    "                layer_dict[new_key] = new_value\n",
    "        torch_tf_mapping.update(layer_dict)\n",
    "\n",
    "    return torch_tf_mapping\n",
    "\n",
    "################################# TF 2 , load pb file and get tensors ##################\n",
    "def get_tf_weights():\n",
    "    import tensorflow as tf\n",
    "    print(\"TensorFlow version:\", tf.__version__)\n",
    "    export_dir = \"model\"\n",
    "    model = tf.saved_model.load(\n",
    "        export_dir, tags=None\n",
    "    )\n",
    "    trainable_variables = model.trainable_variables\n",
    "    trainable_variables_tf = {}\n",
    "    for i, each in enumerate(trainable_variables):\n",
    "        key = each.name\n",
    "        trainable_variables_tf[key] = each.numpy()\n",
    "    return trainable_variables_tf\n",
    "\n",
    "def load_params_on_bert():\n",
    "    trainable_variables_tf_dict = get_tf_weights()\n",
    "    mapping = map_tf_torch_params()\n",
    "    weighted_params = {}\n",
    "    for torch_name, tf in mapping.items():\n",
    "        weighted_params[torch_name] = trainable_variables_tf_dict[tf]\n",
    "\n",
    "    for name, param in bert.named_parameters():\n",
    "        value = weighted_params[name]\n",
    "        param.data = torch.from_numpy(np.asarray(value))\n",
    "    return bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embeddings.word_embeddings.weight',\n",
       " 'embeddings.position_embeddings.weight',\n",
       " 'embeddings.token_type_embeddings.weight',\n",
       " 'embeddings.LayerNorm.weight',\n",
       " 'embeddings.LayerNorm.bias',\n",
       " 'encoder.layers.0.attention.attention.query.weight',\n",
       " 'encoder.layers.0.attention.attention.query.bias',\n",
       " 'encoder.layers.0.attention.attention.key.weight',\n",
       " 'encoder.layers.0.attention.attention.key.bias',\n",
       " 'encoder.layers.0.attention.attention.value.weight',\n",
       " 'encoder.layers.0.attention.attention.value.bias',\n",
       " 'encoder.layers.0.attention.output.dense.weight',\n",
       " 'encoder.layers.0.attention.output.dense.bias',\n",
       " 'encoder.layers.0.attention.output.LayerNorm.weight',\n",
       " 'encoder.layers.0.attention.output.LayerNorm.bias',\n",
       " 'encoder.layers.0.intermediate.dense.weight',\n",
       " 'encoder.layers.0.intermediate.dense.bias',\n",
       " 'encoder.layers.0.output.dense.weight',\n",
       " 'encoder.layers.0.output.dense.bias',\n",
       " 'encoder.layers.0.output.LayerNorm.weight',\n",
       " 'encoder.layers.0.output.LayerNorm.bias',\n",
       " 'pooler_transform/kernel:0',\n",
       " 'pooler_transform/bias:0',\n",
       " 'encoder.layers.1.attention.attention.query.weight',\n",
       " 'encoder.layers.1.attention.attention.query.bias',\n",
       " 'encoder.layers.1.attention.attention.key.weight',\n",
       " 'encoder.layers.1.attention.attention.key.bias',\n",
       " 'encoder.layers.1.attention.attention.value.weight',\n",
       " 'encoder.layers.1.attention.attention.value.bias',\n",
       " 'encoder.layers.1.attention.output.dense.weight',\n",
       " 'encoder.layers.1.attention.output.dense.bias',\n",
       " 'encoder.layers.1.attention.output.LayerNorm.weight',\n",
       " 'encoder.layers.1.attention.output.LayerNorm.bias',\n",
       " 'encoder.layers.1.intermediate.dense.weight',\n",
       " 'encoder.layers.1.intermediate.dense.bias',\n",
       " 'encoder.layers.1.output.dense.weight',\n",
       " 'encoder.layers.1.output.dense.bias',\n",
       " 'encoder.layers.1.output.LayerNorm.weight',\n",
       " 'encoder.layers.1.output.LayerNorm.bias',\n",
       " 'encoder.layers.2.attention.attention.query.weight',\n",
       " 'encoder.layers.2.attention.attention.query.bias',\n",
       " 'encoder.layers.2.attention.attention.key.weight',\n",
       " 'encoder.layers.2.attention.attention.key.bias',\n",
       " 'encoder.layers.2.attention.attention.value.weight',\n",
       " 'encoder.layers.2.attention.attention.value.bias',\n",
       " 'encoder.layers.2.attention.output.dense.weight',\n",
       " 'encoder.layers.2.attention.output.dense.bias',\n",
       " 'encoder.layers.2.attention.output.LayerNorm.weight',\n",
       " 'encoder.layers.2.attention.output.LayerNorm.bias',\n",
       " 'encoder.layers.2.intermediate.dense.weight',\n",
       " 'encoder.layers.2.intermediate.dense.bias',\n",
       " 'encoder.layers.2.output.dense.weight',\n",
       " 'encoder.layers.2.output.dense.bias',\n",
       " 'encoder.layers.2.output.LayerNorm.weight',\n",
       " 'encoder.layers.2.output.LayerNorm.bias',\n",
       " 'encoder.layers.3.attention.attention.query.weight',\n",
       " 'encoder.layers.3.attention.attention.query.bias',\n",
       " 'encoder.layers.3.attention.attention.key.weight',\n",
       " 'encoder.layers.3.attention.attention.key.bias',\n",
       " 'encoder.layers.3.attention.attention.value.weight',\n",
       " 'encoder.layers.3.attention.attention.value.bias',\n",
       " 'encoder.layers.3.attention.output.dense.weight',\n",
       " 'encoder.layers.3.attention.output.dense.bias',\n",
       " 'encoder.layers.3.attention.output.LayerNorm.weight',\n",
       " 'encoder.layers.3.attention.output.LayerNorm.bias',\n",
       " 'encoder.layers.3.intermediate.dense.weight',\n",
       " 'encoder.layers.3.intermediate.dense.bias',\n",
       " 'encoder.layers.3.output.dense.weight',\n",
       " 'encoder.layers.3.output.dense.bias',\n",
       " 'encoder.layers.3.output.LayerNorm.weight',\n",
       " 'encoder.layers.3.output.LayerNorm.bias',\n",
       " 'encoder.layers.4.attention.attention.query.weight',\n",
       " 'encoder.layers.4.attention.attention.query.bias',\n",
       " 'encoder.layers.4.attention.attention.key.weight',\n",
       " 'encoder.layers.4.attention.attention.key.bias',\n",
       " 'encoder.layers.4.attention.attention.value.weight',\n",
       " 'encoder.layers.4.attention.attention.value.bias',\n",
       " 'encoder.layers.4.attention.output.dense.weight',\n",
       " 'encoder.layers.4.attention.output.dense.bias',\n",
       " 'encoder.layers.4.attention.output.LayerNorm.weight',\n",
       " 'encoder.layers.4.attention.output.LayerNorm.bias',\n",
       " 'encoder.layers.4.intermediate.dense.weight',\n",
       " 'encoder.layers.4.intermediate.dense.bias',\n",
       " 'encoder.layers.4.output.dense.weight',\n",
       " 'encoder.layers.4.output.dense.bias',\n",
       " 'encoder.layers.4.output.LayerNorm.weight',\n",
       " 'encoder.layers.4.output.LayerNorm.bias',\n",
       " 'encoder.layers.5.attention.attention.query.weight',\n",
       " 'encoder.layers.5.attention.attention.query.bias',\n",
       " 'encoder.layers.5.attention.attention.key.weight',\n",
       " 'encoder.layers.5.attention.attention.key.bias',\n",
       " 'encoder.layers.5.attention.attention.value.weight',\n",
       " 'encoder.layers.5.attention.attention.value.bias',\n",
       " 'encoder.layers.5.attention.output.dense.weight',\n",
       " 'encoder.layers.5.attention.output.dense.bias',\n",
       " 'encoder.layers.5.attention.output.LayerNorm.weight',\n",
       " 'encoder.layers.5.attention.output.LayerNorm.bias',\n",
       " 'encoder.layers.5.intermediate.dense.weight',\n",
       " 'encoder.layers.5.intermediate.dense.bias',\n",
       " 'encoder.layers.5.output.dense.weight',\n",
       " 'encoder.layers.5.output.dense.bias',\n",
       " 'encoder.layers.5.output.LayerNorm.weight',\n",
       " 'encoder.layers.5.output.LayerNorm.bias',\n",
       " 'encoder.layers.6.attention.attention.query.weight',\n",
       " 'encoder.layers.6.attention.attention.query.bias',\n",
       " 'encoder.layers.6.attention.attention.key.weight',\n",
       " 'encoder.layers.6.attention.attention.key.bias',\n",
       " 'encoder.layers.6.attention.attention.value.weight',\n",
       " 'encoder.layers.6.attention.attention.value.bias',\n",
       " 'encoder.layers.6.attention.output.dense.weight',\n",
       " 'encoder.layers.6.attention.output.dense.bias',\n",
       " 'encoder.layers.6.attention.output.LayerNorm.weight',\n",
       " 'encoder.layers.6.attention.output.LayerNorm.bias',\n",
       " 'encoder.layers.6.intermediate.dense.weight',\n",
       " 'encoder.layers.6.intermediate.dense.bias',\n",
       " 'encoder.layers.6.output.dense.weight',\n",
       " 'encoder.layers.6.output.dense.bias',\n",
       " 'encoder.layers.6.output.LayerNorm.weight',\n",
       " 'encoder.layers.6.output.LayerNorm.bias',\n",
       " 'encoder.layers.7.attention.attention.query.weight',\n",
       " 'encoder.layers.7.attention.attention.query.bias',\n",
       " 'encoder.layers.7.attention.attention.key.weight',\n",
       " 'encoder.layers.7.attention.attention.key.bias',\n",
       " 'encoder.layers.7.attention.attention.value.weight',\n",
       " 'encoder.layers.7.attention.attention.value.bias',\n",
       " 'encoder.layers.7.attention.output.dense.weight',\n",
       " 'encoder.layers.7.attention.output.dense.bias',\n",
       " 'encoder.layers.7.attention.output.LayerNorm.weight',\n",
       " 'encoder.layers.7.attention.output.LayerNorm.bias',\n",
       " 'encoder.layers.7.intermediate.dense.weight',\n",
       " 'encoder.layers.7.intermediate.dense.bias',\n",
       " 'encoder.layers.7.output.dense.weight',\n",
       " 'encoder.layers.7.output.dense.bias',\n",
       " 'encoder.layers.7.output.LayerNorm.weight',\n",
       " 'encoder.layers.7.output.LayerNorm.bias',\n",
       " 'encoder.layers.8.attention.attention.query.weight',\n",
       " 'encoder.layers.8.attention.attention.query.bias',\n",
       " 'encoder.layers.8.attention.attention.key.weight',\n",
       " 'encoder.layers.8.attention.attention.key.bias',\n",
       " 'encoder.layers.8.attention.attention.value.weight',\n",
       " 'encoder.layers.8.attention.attention.value.bias',\n",
       " 'encoder.layers.8.attention.output.dense.weight',\n",
       " 'encoder.layers.8.attention.output.dense.bias',\n",
       " 'encoder.layers.8.attention.output.LayerNorm.weight',\n",
       " 'encoder.layers.8.attention.output.LayerNorm.bias',\n",
       " 'encoder.layers.8.intermediate.dense.weight',\n",
       " 'encoder.layers.8.intermediate.dense.bias',\n",
       " 'encoder.layers.8.output.dense.weight',\n",
       " 'encoder.layers.8.output.dense.bias',\n",
       " 'encoder.layers.8.output.LayerNorm.weight',\n",
       " 'encoder.layers.8.output.LayerNorm.bias',\n",
       " 'encoder.layers.9.attention.attention.query.weight',\n",
       " 'encoder.layers.9.attention.attention.query.bias',\n",
       " 'encoder.layers.9.attention.attention.key.weight',\n",
       " 'encoder.layers.9.attention.attention.key.bias',\n",
       " 'encoder.layers.9.attention.attention.value.weight',\n",
       " 'encoder.layers.9.attention.attention.value.bias',\n",
       " 'encoder.layers.9.attention.output.dense.weight',\n",
       " 'encoder.layers.9.attention.output.dense.bias',\n",
       " 'encoder.layers.9.attention.output.LayerNorm.weight',\n",
       " 'encoder.layers.9.attention.output.LayerNorm.bias',\n",
       " 'encoder.layers.9.intermediate.dense.weight',\n",
       " 'encoder.layers.9.intermediate.dense.bias',\n",
       " 'encoder.layers.9.output.dense.weight',\n",
       " 'encoder.layers.9.output.dense.bias',\n",
       " 'encoder.layers.9.output.LayerNorm.weight',\n",
       " 'encoder.layers.9.output.LayerNorm.bias',\n",
       " 'encoder.layers.10.attention.attention.query.weight',\n",
       " 'encoder.layers.10.attention.attention.query.bias',\n",
       " 'encoder.layers.10.attention.attention.key.weight',\n",
       " 'encoder.layers.10.attention.attention.key.bias',\n",
       " 'encoder.layers.10.attention.attention.value.weight',\n",
       " 'encoder.layers.10.attention.attention.value.bias',\n",
       " 'encoder.layers.10.attention.output.dense.weight',\n",
       " 'encoder.layers.10.attention.output.dense.bias',\n",
       " 'encoder.layers.10.attention.output.LayerNorm.weight',\n",
       " 'encoder.layers.10.attention.output.LayerNorm.bias',\n",
       " 'encoder.layers.10.intermediate.dense.weight',\n",
       " 'encoder.layers.10.intermediate.dense.bias',\n",
       " 'encoder.layers.10.output.dense.weight',\n",
       " 'encoder.layers.10.output.dense.bias',\n",
       " 'encoder.layers.10.output.LayerNorm.weight',\n",
       " 'encoder.layers.10.output.LayerNorm.bias',\n",
       " 'encoder.layers.11.attention.attention.query.weight',\n",
       " 'encoder.layers.11.attention.attention.query.bias',\n",
       " 'encoder.layers.11.attention.attention.key.weight',\n",
       " 'encoder.layers.11.attention.attention.key.bias',\n",
       " 'encoder.layers.11.attention.attention.value.weight',\n",
       " 'encoder.layers.11.attention.attention.value.bias',\n",
       " 'encoder.layers.11.attention.output.dense.weight',\n",
       " 'encoder.layers.11.attention.output.dense.bias',\n",
       " 'encoder.layers.11.attention.output.LayerNorm.weight',\n",
       " 'encoder.layers.11.attention.output.LayerNorm.bias',\n",
       " 'encoder.layers.11.intermediate.dense.weight',\n",
       " 'encoder.layers.11.intermediate.dense.bias',\n",
       " 'encoder.layers.11.output.dense.weight',\n",
       " 'encoder.layers.11.output.dense.bias',\n",
       " 'encoder.layers.11.output.LayerNorm.weight',\n",
       " 'encoder.layers.11.output.LayerNorm.bias']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map_tf_torch_params().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
